{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was used to compare different model architectures based on the test scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_folders = glob.glob(\"../experiment_output_submission/multif0_*exper*\")\n",
    "print(experiment_folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aggregate_df(dataset_fname):\n",
    "    metrics = [\n",
    "        'Precision', 'Recall','Accuracy', 'Substitution Error',\n",
    "        'Miss Error', 'False Alarm Error', 'Total Error', 'Chroma Precision',\n",
    "        'Chroma Recall', 'Chroma Accuracy', 'Chroma Substitution Error',\n",
    "        'Chroma Miss Error', 'Chroma False Alarm Error', 'Chroma Total Error'\n",
    "    ]\n",
    "\n",
    "    scores_mean = {m: [] for m in metrics}\n",
    "    scores_std = {m: [] for m in metrics}\n",
    "    model_key = []\n",
    "    data_frames = []\n",
    "    \n",
    "    experiment_folders = []\n",
    "    for i in range(17):\n",
    "        experiment_folders.append(\"../experiment_output_submission/multif0_exper{}\".format(str(i+1)))\n",
    "        experiment_folders.append(\"../experiment_output_submission/multif0_exper{}_batchin\".format(str(i+1)))\n",
    "    \n",
    "    for folder in experiment_folders:\n",
    "        dataset_scores = os.path.join(folder, dataset_fname)\n",
    "        if os.path.exists(dataset_scores):\n",
    "            model_key.append(folder)\n",
    "            df = pd.read_csv(dataset_scores)\n",
    "            for m in metrics:\n",
    "                scores_mean[m].append(df.ix[1][m])\n",
    "                scores_std[m].append(df.ix[2][m])\n",
    "    \n",
    "    return scores_mean, scores_std, model_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bach10_mean, bach10_std, bach10_models = get_aggregate_df(\"bach10_score_summary.csv\")\n",
    "mdb_mean, mdb_std, mdb_models = get_aggregate_df(\"mdb_test_score_summary.csv\")\n",
    "su_mean, su_std, su_models = get_aggregate_df(\"su_score_summary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 'Accuracy'\n",
    "\n",
    "tick_colors = sns.color_palette('husl', 17)\n",
    "formats = ['o', 'D']\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.title(\"Bach10\")\n",
    "plt.plot([0, 34], [0.758471, 0.758471], '--', label='Benetos')\n",
    "plt.plot([0, 34], [0.681651, 0.681651], '--', label='Duan')\n",
    "x_labs = []\n",
    "for j, k in enumerate(bach10_models):\n",
    "    fmt = formats[j % 2]\n",
    "    if j % 2 == 0:\n",
    "        color = tick_colors[j % 17]\n",
    "    else:\n",
    "        color = tick_colors[(j-1) % 17]\n",
    "    plt.errorbar(j, bach10_mean[m][j], yerr=bach10_std[m][j], fmt=fmt, label=k, color=color)\n",
    "    x_labs.append(k)\n",
    "plt.errorbar(j+1, 0.681651, 0.026199, fmt='s', label='Duan', color='g')\n",
    "plt.errorbar(j+2, 0.758471, 0.026418, fmt='s', label='Benetos', color='b')\n",
    "plt.xticks(range(len(bach10_models)) + [33, 34], x_labs + ['DUAN', 'BENETOS'], rotation='vertical')\n",
    "plt.ylabel(m)\n",
    "plt.legend(ncol=2, bbox_to_anchor=(1, 1))\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../paper-figs/all_results_Bach10.pdf\", format='pdf')\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.title(\"MedleyDB -- Test\")\n",
    "plt.plot([0, 34], [0.297041, 0.297041], '--', label='Benetos')\n",
    "plt.plot([0, 34], [0.234573, 0.234573], '--', label='Duan')\n",
    "x_labs = []\n",
    "for j, k in enumerate(mdb_models):\n",
    "    fmt = formats[j % 2]\n",
    "    if j % 2 == 0:\n",
    "        color = tick_colors[j % 17]\n",
    "    else:\n",
    "        color = tick_colors[(j-1) % 17]\n",
    "    plt.errorbar(j, mdb_mean[m][j], yerr=mdb_std[m][j], fmt=fmt, label=k, color=color)\n",
    "    x_labs.append(k)\n",
    "plt.errorbar(j+1, 0.234573, 0.079084, fmt='s', label='Duan', color='g')\n",
    "plt.errorbar(j+2, 0.297041, 0.114145, fmt='s', label='Benetos', color='b')\n",
    "    \n",
    "plt.xticks(range(len(mdb_models)) + [33, 34], x_labs + ['DUAN', 'BENETOS'], rotation='vertical')\n",
    "plt.ylabel(m)\n",
    "plt.legend(ncol=2, bbox_to_anchor=(1, 1))\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../paper-figs/all_results_mdbtest.pdf\", format='pdf')\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.title(\"Su\")\n",
    "plt.plot([0, 34], [0.355545, 0.355545], '--', label='Benetos')\n",
    "plt.plot([0, 34], [0.317448, 0.317448], '--', label='Duan')\n",
    "x_labs = []\n",
    "for j, k in enumerate(su_models):\n",
    "    fmt = formats[j % 2]\n",
    "    if j % 2 == 0:\n",
    "        color = tick_colors[j % 17]\n",
    "    else:\n",
    "        color = tick_colors[(j-1) % 17]\n",
    "    plt.errorbar(j, su_mean[m][j], yerr=su_std[m][j], fmt=fmt, label=k, color=color)\n",
    "    x_labs.append(k)\n",
    "plt.errorbar(j+1, 0.317448, 0.064461, fmt='s', label='Duan', color='g')\n",
    "plt.errorbar(j+2, 0.355545, 0.051333, fmt='s', label='Benetos', color='b')\n",
    "\n",
    "plt.xticks(range(len(su_models)) + [33, 34], x_labs + ['DUAN', 'BENETOS'], rotation='vertical')\n",
    "plt.ylabel(m)\n",
    "plt.legend(ncol=2, bbox_to_anchor=(1, 1))\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../paper-figs/all_results_su.pdf\", format='pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bach10_mean, bach10_std, bach10_models = get_aggregate_df(\"bach10_score_summary.csv\")\n",
    "mdb_mean, mdb_std, mdb_models = get_aggregate_df(\"mdb_test_score_summary.csv\")\n",
    "su_mean, su_std, su_models = get_aggregate_df(\"su_score_summary.csv\")\n",
    "\n",
    "table = []\n",
    "for b, s, m, key in zip(bach10_mean['Accuracy'], su_mean['Accuracy'], mdb_mean['Accuracy'], bach10_models):\n",
    "    table.append([key, b, s, m])\n",
    "\n",
    "aggregate_df = pd.DataFrame(table, columns=['model', 'Bach10', 'Su', 'MDB'])\n",
    "aggregate_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
